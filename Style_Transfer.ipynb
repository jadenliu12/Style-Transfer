{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d518cb65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "import functools\n",
    "import IPython.display as display\n",
    "from pathlib import Path\n",
    "import random\n",
    "from PIL import Image\n",
    "from matplotlib import pyplot\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (12,12)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79682ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Restrict TensorFlow to only use the fourth GPU\n",
    "        # tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
    "        tf.config.set_visible_devices([], 'GPU') # Apple M-1 Pro bug with tensorflow workaround (use CPU)\n",
    "\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab36a20e",
   "metadata": {},
   "source": [
    "### Visualize Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2dcd30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_img(path_to_img):\n",
    "    max_dim = 512\n",
    "    img = tf.io.read_file(path_to_img)\n",
    "    img = tf.image.decode_image(img, channels=3)\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "\n",
    "    shape = tf.cast(tf.shape(img)[:-1], tf.float32)\n",
    "    long_dim = max(shape)\n",
    "    scale = max_dim / long_dim\n",
    "\n",
    "    new_shape = tf.cast(shape * scale, tf.int32)\n",
    "\n",
    "    img = tf.image.resize(img, new_shape)\n",
    "    # in order to use CNN, add one additional dimension \n",
    "    # to the original image\n",
    "    # img shape: [height, width, channel] -> [batch_size, height, width, channel]\n",
    "    img = img[tf.newaxis, :]\n",
    "    \n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5258161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(image, title=None):\n",
    "    if len(image.shape) > 3:\n",
    "        image = tf.squeeze(image, axis=0)\n",
    "\n",
    "    plt.imshow(image)\n",
    "    if title:\n",
    "        plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b19f6729",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_path = './dataset/content_nthu.jpeg'\n",
    "content_image = load_img(content_path)\n",
    "print('Image shape:', content_image.shape)\n",
    "imshow(content_image, 'Content Image')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115f22fc",
   "metadata": {},
   "source": [
    "### Load Pre-Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a22b3e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.applications.vgg19.preprocess_input(content_image*255)\n",
    "x = tf.image.resize(x, (224, 224))\n",
    "\n",
    "# load pretrained network(VGG19)\n",
    "vgg = tf.keras.applications.VGG19(include_top=True, weights='imagenet')\n",
    "prediction_probabilities = vgg(x)\n",
    "prediction_probabilities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42efd61f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_top_5 = tf.keras.applications.vgg19.decode_predictions(prediction_probabilities.numpy())[0]\n",
    "[(class_name, prob) for (number, class_name, prob) in predicted_top_5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c22fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b592c0b",
   "metadata": {},
   "source": [
    "### Visualize Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39542c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize filter shapes\n",
    "for layer in vgg.layers:\n",
    "    # check for convolutional layer\n",
    "    if 'conv' not in layer.name:\n",
    "        continue\n",
    "    # get filter weights\n",
    "    filters, biases = layer.get_weights()\n",
    "    print(layer.name, filters.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775d98f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "\n",
    "# retrieve weights from the second hidden layer\n",
    "filters, biases = vgg.layers[1].get_weights()\n",
    "\n",
    "# normalize filter values to 0-1 so we can visualize them\n",
    "f_min, f_max = filters.min(), filters.max()\n",
    "filters = (filters - f_min) / (f_max - f_min)\n",
    "\n",
    "# plot first few filters\n",
    "n_filters, ix = 64, 1\n",
    "\n",
    "for i in range(n_filters):\n",
    "    # get the filter\n",
    "    f = filters[:, :, :, i]\n",
    "    # plot each channel separately\n",
    "    for j in range(3):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = pyplot.subplot(14, 14, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        pyplot.imshow(f[:, :, j], cmap='gray')\n",
    "        ix += 1\n",
    "        \n",
    "# show the figure\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22289455",
   "metadata": {},
   "source": [
    "### Visualize Feature Maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cd50bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,16))\n",
    "\n",
    "# redefine model to output right after the first hidden layer\n",
    "model = tf.keras.Model(inputs=[vgg.input], outputs=vgg.layers[1].output)\n",
    "model.summary()\n",
    "\n",
    "# preprocess input\n",
    "content_image = tf.keras.applications.vgg19.preprocess_input(content_image*255)\n",
    "content_image = tf.image.resize(content_image, (224, 224))\n",
    "\n",
    "# get feature map for first hidden layer\n",
    "feature_maps = model.predict(content_image)\n",
    "\n",
    "# plot all 64 maps in an 8x8 squares\n",
    "square = 8\n",
    "ix = 1\n",
    "for _ in range(square):\n",
    "    for _ in range(square):\n",
    "        # specify subplot and turn of axis\n",
    "        ax = pyplot.subplot(square, square, ix)\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        # plot filter channel in grayscale\n",
    "        pyplot.imshow(feature_maps[0, :, :, ix-1], cmap='gray')\n",
    "        ix += 1\n",
    "        \n",
    "# show the figure\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e7af52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get feature maps for last convolutional layer in each block\n",
    "ixs = [2, 5, 10, 15, 20]\n",
    "outputs = [vgg.layers[i].output for i in ixs]\n",
    "model = tf.keras.Model(inputs=[vgg.input], outputs=outputs)\n",
    "feature_maps = model.predict(content_image)\n",
    "\n",
    "# plot the output from each block\n",
    "square = 8\n",
    "for i, fmap in enumerate(feature_maps):\n",
    "    # plot all 64 maps in an 8x8 squares\n",
    "    ix = 1\n",
    "    print(outputs[i])\n",
    "    plt.figure(figsize=(16,16))\n",
    "    for _ in range(square):\n",
    "        for _ in range(square):\n",
    "            # specify subplot and turn of axis\n",
    "            ax = pyplot.subplot(square, square, ix)\n",
    "            ax.set_xticks([])\n",
    "            ax.set_yticks([])\n",
    "            # plot filter channel in grayscale\n",
    "            pyplot.imshow(fmap[0, :, :, ix-1], cmap='gray')\n",
    "            ix += 1\n",
    "            \n",
    "    # show the figure\n",
    "    pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d772ebd7",
   "metadata": {},
   "source": [
    "### Visualize Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cb08762",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_layers(layer_names):\n",
    "    \"\"\" Creates a vgg model that returns a list of intermediate output values.\"\"\"\n",
    "    # Load our model. Load pretrained VGG, trained on imagenet data\n",
    "    vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "    vgg.trainable = False\n",
    "\n",
    "    outputs = [vgg.get_layer(name).output for name in layer_names]\n",
    "\n",
    "    model = tf.keras.Model([vgg.input], outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a936ecb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientModel(tf.keras.models.Model):\n",
    "    def __init__(self, layers):\n",
    "        super(GradientModel, self).__init__()\n",
    "        self.vgg =  vgg_layers(layers)\n",
    "        self.num_style_layers = len(layers)\n",
    "        self.vgg.trainable = False\n",
    "        \n",
    "    # return the feature map of required layer\n",
    "    def call(self, inputs):\n",
    "        \"Expects float input in [0,1]\"\n",
    "        inputs = inputs*255.0\n",
    "        preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n",
    "        outputs = self.vgg(preprocessed_input)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332f9966",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def visualize_gradient(image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        feature = extractor(image)\n",
    "        # grad = d_feature/d_image\n",
    "        grad = tape.gradient(tf.reduce_max(feature, axis=3), image)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a68bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_image = load_img(content_path)\n",
    "\n",
    "# activation layer\n",
    "layers = ['block4_conv2']\n",
    "image = tf.Variable(content_image)\n",
    "\n",
    "extractor = GradientModel(layers)\n",
    "grad = visualize_gradient(image)\n",
    "\n",
    "# look at the range of gradients\n",
    "print(\"shape: \", grad.numpy().shape)\n",
    "print(\"min: \", grad.numpy().min())\n",
    "print(\"max: \", grad.numpy().max())\n",
    "print(\"mean: \", grad.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c8a8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize filter values to 0-1 so we can visualize them\n",
    "g_min, g_max = grad.numpy().min(), grad.numpy().max()\n",
    "filters = (grad - g_min) / (g_max - g_min)\n",
    "\n",
    "plt.figure(figsize=(14,10))\n",
    "plt.subplot(1, 2, 1)\n",
    "imshow(image.read_value()[0])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "imshow(filters[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87322c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def visualize_gradient_single_layer(image, layer_i):\n",
    "    with tf.GradientTape() as tape:\n",
    "        feature = extractor(image)\n",
    "        grad = tape.gradient(tf.reduce_mean(feature[:, :, :, layer_i]), image)\n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cc27c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(14,10))\n",
    "\n",
    "grad = visualize_gradient_single_layer(image, 77)\n",
    "\n",
    "# normalize filter values to 0-1 so we can visualize them\n",
    "g_min, g_max = grad.numpy().min(), grad.numpy().max()\n",
    "filters = (grad - g_min) / (g_max - g_min)\n",
    "\n",
    "plt.figure(figsize=(14,10))\n",
    "plt.subplot(1, 2, 1)\n",
    "imshow(image.read_value()[0])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "imshow(filters[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303b831f",
   "metadata": {},
   "source": [
    "### Guided Backpropagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1203f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from guided_backprop import GuidedBackprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fee1d17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.keras.applications.vgg19.preprocess_input(content_image*255)\n",
    "x = tf.image.resize(x, (224, 224))\n",
    "\n",
    "backprop_vgg = GuidedBackprop(model=vgg, layerName='predictions')\n",
    "grad = backprop_vgg.guided_backprop(x)[0].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab8fcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the original image and the three saliency map variants\n",
    "plt.figure(figsize=(16, 16), facecolor='w')\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.title('Input')\n",
    "plt.imshow(tf.image.resize(content_image, (224, 224))[0])\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.title('Abs. saliency')\n",
    "plt.imshow(np.abs(grad).max(axis=-1), cmap='gray')\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.title('Pos. saliency')\n",
    "plt.imshow((np.maximum(0, grad) / grad.max()))\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.title('Neg. saliency')\n",
    "plt.imshow((np.maximum(0, -grad) / -grad.min()))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8de5d5",
   "metadata": {},
   "source": [
    "### A Neural Algorithm of Artistic Style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1896e6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "content_path = './dataset/content_nthu.jpg'\n",
    "style_path = './dataset/style_starry_night.jpg'\n",
    "\n",
    "content_image = load_img(content_path)\n",
    "style_image = load_img(style_path)\n",
    "\n",
    "plt.figure(figsize=(14,10))\n",
    "plt.subplot(1, 2, 1)\n",
    "imshow(content_image, 'Content Image')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "imshow(style_image, 'Style Image')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e4b129",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "\n",
    "print()\n",
    "for layer in vgg.layers:\n",
    "    print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e9d509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content layer where will pull our feature maps\n",
    "content_layers = ['block5_conv2'] \n",
    "\n",
    "# Style layer of interest\n",
    "style_layers = ['block1_conv1',\n",
    "                'block2_conv1',\n",
    "                'block3_conv1', \n",
    "                'block4_conv1', \n",
    "                'block5_conv1']\n",
    "\n",
    "num_content_layers = len(content_layers)\n",
    "num_style_layers = len(style_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17289cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vgg_layers(layer_names):\n",
    "    \"\"\" Creates a vgg model that returns a list of intermediate output values.\"\"\"\n",
    "    # Load our model. Load pretrained VGG, trained on imagenet data\n",
    "    vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "    vgg.trainable = False\n",
    "\n",
    "    outputs = [vgg.get_layer(name).output for name in layer_names]\n",
    "\n",
    "    model = tf.keras.Model([vgg.input], outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a2893f",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_extractor = vgg_layers(style_layers)\n",
    "style_outputs = style_extractor(style_image*255)\n",
    "\n",
    "#Look at the statistics of each layer's output\n",
    "for name, output in zip(style_layers, style_outputs):\n",
    "    print(name)\n",
    "    print(\"  shape: \", output.numpy().shape)\n",
    "    print(\"  min: \", output.numpy().min())\n",
    "    print(\"  max: \", output.numpy().max())\n",
    "    print(\"  mean: \", output.numpy().mean())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9780a1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate style\n",
    "def gram_matrix(input_tensor):\n",
    "    result = tf.linalg.einsum('bijc,bijd->bcd', input_tensor, input_tensor)\n",
    "    input_shape = tf.shape(input_tensor)\n",
    "    num_locations = tf.cast(input_shape[1]*input_shape[2], tf.float32)\n",
    "    return result/(num_locations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9e672a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract style and content\n",
    "class StyleContentModel(tf.keras.models.Model):\n",
    "    def __init__(self, style_layers, content_layers):\n",
    "        super(StyleContentModel, self).__init__()\n",
    "        self.vgg =  vgg_layers(style_layers + content_layers)\n",
    "        self.style_layers = style_layers\n",
    "        self.content_layers = content_layers\n",
    "        self.num_style_layers = len(style_layers)\n",
    "        self.vgg.trainable = False\n",
    "\n",
    "    def call(self, inputs):\n",
    "        \"Expects float input in [0,1]\"\n",
    "        inputs = inputs*255.0\n",
    "        preprocessed_input = tf.keras.applications.vgg19.preprocess_input(inputs)\n",
    "        outputs = self.vgg(preprocessed_input)\n",
    "        style_outputs, content_outputs = (outputs[:self.num_style_layers], \n",
    "                                          outputs[self.num_style_layers:])\n",
    "\n",
    "        style_outputs = [gram_matrix(style_output)\n",
    "                         for style_output in style_outputs]\n",
    "\n",
    "        content_dict = {content_name:value \n",
    "                        for content_name, value \n",
    "                        in zip(self.content_layers, content_outputs)}\n",
    "\n",
    "        style_dict = {style_name:value\n",
    "                      for style_name, value\n",
    "                      in zip(self.style_layers, style_outputs)}\n",
    "\n",
    "        return {'content':content_dict, 'style':style_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bada37",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = StyleContentModel(style_layers, content_layers)\n",
    "\n",
    "results = extractor(tf.constant(content_image))\n",
    "\n",
    "style_results = results['style']\n",
    "\n",
    "print('Styles:')\n",
    "for name, output in sorted(results['style'].items()):\n",
    "    print(\"  \", name)\n",
    "    print(\"    shape: \", output.numpy().shape)\n",
    "    print(\"    min: \", output.numpy().min())\n",
    "    print(\"    max: \", output.numpy().max())\n",
    "    print(\"    mean: \", output.numpy().mean())\n",
    "    print()\n",
    "\n",
    "print(\"Contents:\")\n",
    "for name, output in sorted(results['content'].items()):\n",
    "    print(\"  \", name)\n",
    "    print(\"    shape: \", output.numpy().shape)\n",
    "    print(\"    min: \", output.numpy().min())\n",
    "    print(\"    max: \", output.numpy().max())\n",
    "    print(\"    mean: \", output.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a3ba0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_content_loss(outputs):\n",
    "    style_outputs = outputs['style']\n",
    "    content_outputs = outputs['content']\n",
    "    style_loss = tf.add_n([tf.reduce_mean((style_outputs[name]-style_targets[name])**2) \n",
    "                           for name in style_outputs.keys()])\n",
    "    style_loss *= style_weight / num_style_layers\n",
    "\n",
    "    content_loss = tf.add_n([tf.reduce_mean((content_outputs[name]-content_targets[name])**2) \n",
    "                             for name in content_outputs.keys()])\n",
    "    content_loss *= content_weight / num_content_layers\n",
    "    loss = style_loss + content_loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f545c030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_0_1(image):\n",
    "    return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10d223c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = extractor(image)\n",
    "        loss = style_content_loss(outputs)\n",
    "\n",
    "    # tape.gradient: d_loss/d_image\n",
    "    grad = tape.gradient(loss, image)\n",
    "    opt.apply_gradients([(grad, image)])\n",
    "    image.assign(clip_0_1(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b941400",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_targets = extractor(style_image)['style']\n",
    "content_targets = extractor(content_image)['content']\n",
    "\n",
    "image = tf.Variable(content_image)\n",
    "opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n",
    "style_weight = 1600    # Change it as you want\n",
    "content_weight = 1600  # Change it as you want\n",
    "\n",
    "train_step(image)\n",
    "train_step(image)\n",
    "train_step(image)\n",
    "plt.imshow(image.read_value()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84677fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "epochs = 10\n",
    "steps_per_epoch = 100\n",
    "\n",
    "step = 0\n",
    "for n in range(epochs):\n",
    "    for m in range(steps_per_epoch):\n",
    "        step += 1\n",
    "        train_step(image)\n",
    "    imshow(image.read_value())\n",
    "    plt.title(\"Train step: {}\".format(step))\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "end = time.time()\n",
    "print(\"Total time: {:.1f}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744cf298",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total variation loss\n",
    "plt.figure(figsize=(14,10))\n",
    "\n",
    "sobel = tf.image.sobel_edges(content_image)\n",
    "plt.subplot(1,2,1)\n",
    "imshow(clip_0_1(sobel[...,0]/4+0.5), \"Horizontal Sobel-edges\")\n",
    "plt.subplot(1,2,2)\n",
    "imshow(clip_0_1(sobel[...,1]/4+0.5), \"Vertical Sobel-edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273b36b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_variation_loss(image):\n",
    "    pixel_dif1 = image[1:, :, :] - image[:-1, :, :]\n",
    "    pixel_dif2 = image[:, 1:, :] - image[:, :-1, :]\n",
    "    \n",
    "    tot_var = (tf.math.reduce_sum(tf.math.abs(pixel_dif1)) + tf.math.reduce_sum(tf.math.abs(pixel_dif2)))\n",
    "\n",
    "    return tot_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9536ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_variation_weight = 3200 # Change it as you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd811f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = extractor(image)\n",
    "        loss = style_content_loss(outputs)\n",
    "        loss += total_variation_weight*total_variation_loss(image)\n",
    "\n",
    "    grad = tape.gradient(loss, image)\n",
    "    opt.apply_gradients([(grad, image)])\n",
    "    image.assign(clip_0_1(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08266de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.Variable(content_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c902057",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "epochs = 10\n",
    "steps_per_epoch = 100\n",
    "\n",
    "step = 0\n",
    "for n in range(epochs):\n",
    "    for m in range(steps_per_epoch):\n",
    "        step += 1\n",
    "        train_step(image)\n",
    "    imshow(image.read_value())\n",
    "    plt.title(\"Train step: {}\".format(step))\n",
    "    plt.show()\n",
    "\n",
    "end = time.time()\n",
    "print(\"Total time: {:.1f}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345d7993",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = './dataset/style_transfer_nthu_starry_night.png'\n",
    "mpl.image.imsave(file_name, image[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5887b0",
   "metadata": {},
   "source": [
    "For the above codes I've tried different values for style, content and total_variation weight.\n",
    "\n",
    "\n",
    "For style and content weight experiments, i've tried these values:\n",
    "- style_weight and content_weight = 10\n",
    "- style_weight and content_weight = 100\n",
    "- style_weight and content_weight = 200\n",
    "- style_weight and content_weight = 1600\n",
    "\n",
    "And from those experiments I've found out that the greater the weight the better the transfer style became\n",
    "\n",
    "\n",
    "For total_variation weight experimets, i've tried these values:\n",
    "- total_variation_weight = 100\n",
    "- total_variation_weight = 1000\n",
    "- total_variation_weight = 3200  \n",
    "\n",
    "Frome these experiments I didn't find that much improvement from using different values of total_variation_weight but at the end i've decided to use 3200, but compared to when using style and content weight we can see that there is an improvement in the style transfer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff55edfc",
   "metadata": {},
   "source": [
    "### Try different layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aa8554",
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg = tf.keras.applications.VGG19(include_top=False, weights='imagenet')\n",
    "\n",
    "print()\n",
    "for layer in vgg.layers:\n",
    "    print(layer.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "945e7e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Content layer where will pull our feature maps\n",
    "content_layers = ['block2_conv2'] \n",
    "\n",
    "# Style layer of interest\n",
    "style_layers = ['block1_conv1',\n",
    "                'block2_conv1',\n",
    "                'block3_conv1',\n",
    "                'block3_conv3',\n",
    "                'block4_conv1',\n",
    "                'block4_conv3',\n",
    "                'block5_conv1',\n",
    "                'block5_conv3',]\n",
    "\n",
    "num_content_layers = len(content_layers)\n",
    "num_style_layers = len(style_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96002e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_extractor = vgg_layers(style_layers)\n",
    "style_outputs = style_extractor(style_image*255)\n",
    "\n",
    "#Look at the statistics of each layer's output\n",
    "for name, output in zip(style_layers, style_outputs):\n",
    "    print(name)\n",
    "    print(\"  shape: \", output.numpy().shape)\n",
    "    print(\"  min: \", output.numpy().min())\n",
    "    print(\"  max: \", output.numpy().max())\n",
    "    print(\"  mean: \", output.numpy().mean())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e99f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor = StyleContentModel(style_layers, content_layers)\n",
    "\n",
    "results = extractor(tf.constant(content_image))\n",
    "\n",
    "style_results = results['style']\n",
    "\n",
    "print('Styles:')\n",
    "for name, output in sorted(results['style'].items()):\n",
    "    print(\"  \", name)\n",
    "    print(\"    shape: \", output.numpy().shape)\n",
    "    print(\"    min: \", output.numpy().min())\n",
    "    print(\"    max: \", output.numpy().max())\n",
    "    print(\"    mean: \", output.numpy().mean())\n",
    "    print()\n",
    "\n",
    "print(\"Contents:\")\n",
    "for name, output in sorted(results['content'].items()):\n",
    "    print(\"  \", name)\n",
    "    print(\"    shape: \", output.numpy().shape)\n",
    "    print(\"    min: \", output.numpy().min())\n",
    "    print(\"    max: \", output.numpy().max())\n",
    "    print(\"    mean: \", output.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6375b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function()\n",
    "def train_step(image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = extractor(image)\n",
    "        loss = style_content_loss(outputs)\n",
    "\n",
    "    # tape.gradient: d_loss/d_image\n",
    "    grad = tape.gradient(loss, image)\n",
    "    opt.apply_gradients([(grad, image)])\n",
    "    image.assign(clip_0_1(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc904097",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_targets = extractor(style_image)['style']\n",
    "content_targets = extractor(content_image)['content']\n",
    "\n",
    "image = tf.Variable(content_image)\n",
    "opt = tf.optimizers.Adam(learning_rate=0.02, beta_1=0.99, epsilon=1e-1)\n",
    "style_weight = 1600    # Change it as you want\n",
    "content_weight = 1600  # Change it as you want\n",
    "\n",
    "train_step(image)\n",
    "train_step(image)\n",
    "train_step(image)\n",
    "plt.imshow(image.read_value()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecf92f17",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "epochs = 10\n",
    "steps_per_epoch = 100\n",
    "\n",
    "step = 0\n",
    "for n in range(epochs):\n",
    "    for m in range(steps_per_epoch):\n",
    "        step += 1\n",
    "        train_step(image)\n",
    "    imshow(image.read_value())\n",
    "    plt.title(\"Train step: {}\".format(step))\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "end = time.time()\n",
    "print(\"Total time: {:.1f}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eceac7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Total variation loss\n",
    "plt.figure(figsize=(14,10))\n",
    "\n",
    "sobel = tf.image.sobel_edges(content_image)\n",
    "plt.subplot(1,2,1)\n",
    "imshow(clip_0_1(sobel[...,0]/4+0.5), \"Horizontal Sobel-edges\")\n",
    "plt.subplot(1,2,2)\n",
    "imshow(clip_0_1(sobel[...,1]/4+0.5), \"Vertical Sobel-edges\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a4b894",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_variation_weight = 3200 # Change it as you want\n",
    "\n",
    "@tf.function()\n",
    "def train_step(image):\n",
    "    with tf.GradientTape() as tape:\n",
    "        outputs = extractor(image)\n",
    "        loss = style_content_loss(outputs)\n",
    "        loss += total_variation_weight*total_variation_loss(image)\n",
    "\n",
    "    grad = tape.gradient(loss, image)\n",
    "    opt.apply_gradients([(grad, image)])\n",
    "    image.assign(clip_0_1(image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee4e74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = tf.Variable(content_image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04368442",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "epochs = 10\n",
    "steps_per_epoch = 100\n",
    "\n",
    "step = 0\n",
    "for n in range(epochs):\n",
    "    for m in range(steps_per_epoch):\n",
    "        step += 1\n",
    "        train_step(image)\n",
    "    imshow(image.read_value())\n",
    "    plt.title(\"Train step: {}\".format(step))\n",
    "    plt.show()\n",
    "\n",
    "end = time.time()\n",
    "print(\"Total time: {:.1f}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51c4268f",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = './dataset/style_transfer_nthu_starry_night_2.png'\n",
    "mpl.image.imsave(file_name, image[0].numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47010fb",
   "metadata": {},
   "source": [
    "After experimenting with different layers, I can conclude that adding more than 1 content layer wouldn't do much and won't cause much improvement. Meanwhile, by adding or changing style layers we can get some improvement in the style transfer and obtain a much better image. But I think the weights of the style/content/total variation is a more important parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9dba09d",
   "metadata": {},
   "source": [
    "### AdaIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c4a1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "CONTENT_DIRS = ['./dataset/mscoco/test2014']\n",
    "STYLE_DIRS = ['./dataset/wikiart/test']\n",
    "\n",
    "# VGG19 was trained by Caffe which converted images from RGB to BGR,\n",
    "# then zero-centered each color channel with respect to the ImageNet \n",
    "# dataset, without scaling.  \n",
    "IMG_MEANS = np.array([103.939, 116.779, 123.68]) # BGR\n",
    "\n",
    "IMG_SHAPE = (224, 224, 3) # training image shape, (h, w, c)\n",
    "SHUFFLE_BUFFER = 1000\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 30\n",
    "STEPS_PER_EPOCH = 12000 // BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8302fe2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sample_files(dir, num, pattern='**/*.jpg'):\n",
    "    '''Samples files in a directory using the reservoir sampling.'''\n",
    "\n",
    "    paths = Path(dir).glob(pattern) # list of Path objects\n",
    "    sampled = []\n",
    "    for i, path in enumerate(paths):\n",
    "        if i < num:\n",
    "            sampled.append(path) \n",
    "        else:\n",
    "            s = random.randint(0, i)\n",
    "            if s < num:\n",
    "                sampled[s] = path\n",
    "    return sampled\n",
    "\n",
    "def plot_images(dir, row, col, pattern):\n",
    "    paths = sample_files(dir, row*col, pattern)\n",
    "\n",
    "    plt.figure(figsize=(2*col, 2*row))\n",
    "    for i in range(row*col):\n",
    "        im = Image.open(paths[i])\n",
    "        w, h = im.size\n",
    "\n",
    "        plt.subplot(row, col, i+1)\n",
    "        plt.imshow(im)\n",
    "        plt.grid(False)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.xlabel(f'{w}x{h}')\n",
    "    plt.show()\n",
    "\n",
    "print('Sampled content images:')\n",
    "plot_images(CONTENT_DIRS[0], 4, 8, pattern='*.jpg')\n",
    "\n",
    "print('Sampled style images:')\n",
    "plot_images(STYLE_DIRS[0], 4, 8, pattern='*.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaabe43",
   "metadata": {},
   "source": [
    "Due to ipynb file size is too big to be uploaded to eeclass, I've decided to clear the output of these sampled content and style images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bc0f35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(dir_path, min_shape=None):\n",
    "    paths = Path(dir_path).glob('**/*.jpg')\n",
    "    deleted  = 0\n",
    "    for path in paths:\n",
    "        try:\n",
    "            # Make sure we can decode the image\n",
    "            im = tf.io.read_file(str(path.resolve()))\n",
    "            im = tf.image.decode_jpeg(im)\n",
    "\n",
    "            # Remove grayscale images \n",
    "            shape = im.shape\n",
    "            if shape[2] < 3:\n",
    "                path.unlink()\n",
    "                deleted += 1\n",
    "\n",
    "            # Remove small images\n",
    "            if min_shape is not None:\n",
    "                if shape[0] < min_shape[0] or shape[1] < min_shape[1]:\n",
    "                    path.unlink()\n",
    "                    deleted += 1\n",
    "        except Exception as e:\n",
    "            path.unlink()\n",
    "            deleted += 1\n",
    "    return deleted\n",
    "\n",
    "for dir in CONTENT_DIRS:\n",
    "    deleted = clean(dir)\n",
    "print(f'#Deleted content images: {deleted}')\n",
    "\n",
    "for dir in STYLE_DIRS:\n",
    "    deleted = clean(dir)\n",
    "print(f'#Deleted style images: {deleted}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562cfb73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(path, init_shape=(448, 448)):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, init_shape)\n",
    "    image = tf.image.random_crop(image, size=IMG_SHAPE)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    \n",
    "    # Convert image from RGB to BGR, then zero-center each color channel with\n",
    "    # respect to the ImageNet dataset, without scaling.\n",
    "    image = image[..., ::-1] # RGB to BGR\n",
    "    image -= (103.939, 116.779, 123.68) # BGR means\n",
    "    return image\n",
    "\n",
    "def np_image(image):\n",
    "    image += (103.939, 116.779, 123.68) # BGR means\n",
    "    image = image[..., ::-1] # BGR to RGB\n",
    "    image = tf.clip_by_value(image, 0, 255)\n",
    "    image = tf.cast(image, dtype='uint8')\n",
    "    return image.numpy()\n",
    "\n",
    "def build_dataset(num_gpus=1):\n",
    "    c_paths = []\n",
    "    for c_dir in CONTENT_DIRS:\n",
    "        c_paths += Path(c_dir).glob('*.jpg')\n",
    "    c_paths = [str(path.resolve()) for path in c_paths]\n",
    "    s_paths = []\n",
    "    for s_dir in STYLE_DIRS:\n",
    "        s_paths += Path(s_dir).glob('*.jpg')\n",
    "    s_paths = [str(path.resolve()) for path in s_paths]\n",
    "    print(f'Building dataset from {len(c_paths):,} content images and {len(s_paths):,} style images... ', end='')\n",
    "\n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "    c_ds = tf.data.Dataset.from_tensor_slices(c_paths)\n",
    "    c_ds = c_ds.map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "    c_ds = c_ds.repeat()\n",
    "    c_ds = c_ds.shuffle(buffer_size=SHUFFLE_BUFFER)\n",
    "\n",
    "    s_ds = tf.data.Dataset.from_tensor_slices(s_paths)\n",
    "    s_ds = s_ds.map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "    s_ds = s_ds.repeat()\n",
    "    s_ds = s_ds.shuffle(buffer_size=SHUFFLE_BUFFER)\n",
    "\n",
    "    ds = tf.data.Dataset.zip((c_ds, s_ds))\n",
    "    ds = ds.batch(BATCH_SIZE * num_gpus)\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "\n",
    "    print('done')\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0836ee89",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = build_dataset()\n",
    "c_batch, s_batch = next(iter(ds.take(1)))\n",
    "\n",
    "print('Content batch shape:', c_batch.shape)\n",
    "print('Style batch shape:', s_batch.shape)\n",
    "\n",
    "plt.figure(figsize=(8, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(np_image(c_batch[0]))\n",
    "plt.grid(False)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel('Content')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(np_image(s_batch[0]))\n",
    "plt.grid(False)\n",
    "plt.xticks([])\n",
    "plt.yticks([])\n",
    "plt.xlabel('Style')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5115bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaIN(tf.keras.layers.Layer):\n",
    "    def __init__(self, name):\n",
    "        super(AdaIN, self).__init__()\n",
    "        self.epsilon = 1e-5\n",
    "    \n",
    "    def get_std(self, value):\n",
    "        mean, variance = tf.nn.moments(value, axes=(1,2), keepdims=True)\n",
    "        standard_deviation = tf.sqrt(variance + self.epsilon)\n",
    "        \n",
    "        return mean, standard_deviation\n",
    "\n",
    "    def call(self, inputs):\n",
    "        content_mean, content_std = self.get_std(inputs[0])\n",
    "        style_mean, style_std = self.get_std(inputs[1])\n",
    "        t = style_std * (inputs[0] - content_mean) / content_std + style_mean\n",
    "        \n",
    "        return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dc5319",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArbitraryStyleTransferNet(tf.keras.Model):\n",
    "    CONTENT_LAYER = 'block4_conv1'\n",
    "    STYLE_LAYERS = ('block1_conv1', 'block2_conv1', 'block3_conv1', 'block4_conv1')\n",
    "\n",
    "    @staticmethod\n",
    "    def declare_decoder():\n",
    "        a_input = tf.keras.Input(shape=(28, 28, 512), name='input_adain')\n",
    "\n",
    "        h = tf.keras.layers.Conv2DTranspose(256, 3, padding='same', activation='relu')(a_input)\n",
    "        h = tf.keras.layers.UpSampling2D(2)(h)\n",
    "        h = tf.keras.layers.Conv2DTranspose(256, 3, padding='same', activation='relu')(h)\n",
    "        h = tf.keras.layers.Conv2DTranspose(256, 3, padding='same', activation='relu')(h)\n",
    "        h = tf.keras.layers.Conv2DTranspose(256, 3, padding='same', activation='relu')(h)\n",
    "        h = tf.keras.layers.Conv2DTranspose(128, 3, padding='same', activation='relu')(h)\n",
    "        h = tf.keras.layers.UpSampling2D(2)(h)\n",
    "        h = tf.keras.layers.Conv2DTranspose(128, 3, padding='same', activation='relu')(h)\n",
    "        h = tf.keras.layers.Conv2DTranspose(64, 3, padding='same', activation='relu')(h)\n",
    "        h = tf.keras.layers.UpSampling2D(2)(h)\n",
    "        h = tf.keras.layers.Conv2DTranspose(64, 3, padding='same', activation='relu')(h)\n",
    "        output = tf.keras.layers.Conv2DTranspose(3, 3, padding='same')(h)\n",
    "\n",
    "        return tf.keras.Model(inputs=a_input, outputs=output, name='decoder')\n",
    "  \n",
    "    def __init__(self,\n",
    "                 img_shape=(224, 224, 3),\n",
    "                 content_loss_weight=1,\n",
    "                 style_loss_weight=10,\n",
    "                 name='arbitrary_style_transfer_net',\n",
    "                 **kwargs):\n",
    "        super(ArbitraryStyleTransferNet, self).__init__(name=name, **kwargs)\n",
    "\n",
    "        self.img_shape = img_shape\n",
    "        self.content_loss_weight = content_loss_weight\n",
    "        self.style_loss_weight = style_loss_weight\n",
    "        \n",
    "        vgg19 = tf.keras.applications.VGG19(include_top=False, weights='imagenet', input_shape=img_shape)\n",
    "        vgg19.trainable = False\n",
    "\n",
    "        c_output = [vgg19.get_layer(ArbitraryStyleTransferNet.CONTENT_LAYER).output]\n",
    "        s_outputs = [vgg19.get_layer(name).output for name in ArbitraryStyleTransferNet.STYLE_LAYERS]\n",
    "        self.vgg19 = tf.keras.Model(inputs=vgg19.input, outputs=c_output+s_outputs, name='vgg19')\n",
    "        self.vgg19.trainable = False\n",
    "\n",
    "        self.adain = AdaIN(name='adain')\n",
    "        self.decoder = ArbitraryStyleTransferNet.declare_decoder()\n",
    "  \n",
    "    def call(self, inputs):\n",
    "        c_batch, s_batch = inputs\n",
    "\n",
    "        c_enc = self.vgg19(c_batch)\n",
    "        c_enc_c = c_enc[0]\n",
    "\n",
    "        s_enc = self.vgg19(s_batch)\n",
    "        s_enc_c = s_enc[0]\n",
    "        s_enc_s = s_enc[1:] \n",
    "        \n",
    "        # normalized_c is the output of AdaIN layer\n",
    "        normalized_c = self.adain((c_enc_c, s_enc_c))\n",
    "        output = self.decoder(normalized_c)\n",
    "\n",
    "        # Calculate loss\n",
    "        out_enc = self.vgg19(output)\n",
    "        out_enc_c = out_enc[0]\n",
    "        out_enc_s = out_enc[1:]\n",
    "\n",
    "        loss_c = tf.reduce_mean(tf.math.squared_difference(out_enc_c, normalized_c))\n",
    "        self.add_loss(self.content_loss_weight * loss_c)\n",
    "        \n",
    "        loss_s = 0\n",
    "        for o, s in zip(out_enc_s, s_enc_s):    \n",
    "            o_mean, o_var = tf.nn.moments(o, axes=(1,2), keepdims=True)\n",
    "            o_std = tf.sqrt(o_var + self.adain.epsilon)\n",
    "\n",
    "            s_mean, s_var = tf.nn.moments(s, axes=(1,2), keepdims=True)\n",
    "            s_std = tf.sqrt(s_var + self.adain.epsilon)\n",
    "\n",
    "            loss_mean = tf.reduce_mean(tf.math.squared_difference(o_mean, s_mean))\n",
    "            loss_std = tf.reduce_mean(tf.math.squared_difference(o_std, s_std))\n",
    "\n",
    "            loss_s += loss_mean + loss_std\n",
    "        self.add_loss(self.style_loss_weight * loss_s)\n",
    "\n",
    "        return output, c_enc_c, normalized_c, out_enc_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ccb63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "def plot_outputs(outputs, captions=None, col=5):\n",
    "    row = len(outputs)\n",
    "    plt.figure(figsize=(3*col, 3*row))\n",
    "    for i in range(col):\n",
    "        for j in range(row):\n",
    "            plt.subplot(row, col, j*col+i+1)\n",
    "            plt.imshow(np_image(outputs[j][i,...,:3]))\n",
    "            plt.grid(False)\n",
    "            plt.xticks([])\n",
    "            plt.yticks([])\n",
    "            if captions is not None:\n",
    "                plt.xlabel(captions[j])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad740a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = build_dataset()\n",
    "model = ArbitraryStyleTransferNet(img_shape=IMG_SHAPE)\n",
    "\n",
    "c_batch, s_batch = next(iter(ds.take(1)))\n",
    "print(f'Input shape: ({c_batch.shape}, {s_batch.shape})')\n",
    "output, *_ = model((c_batch, s_batch))\n",
    "print(f'Output shape: {output.shape}')\n",
    "print(f'Init. content loss: {model.losses[0]:,.2f}, style loss: {model.losses[1]:,.2f}')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa724448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "optimizer = tf.keras.optimizers.Adam(lr=5e-4)\n",
    "c_loss_metric, s_loss_metric = tf.keras.metrics.Mean(), tf.keras.metrics.Mean()\n",
    "\n",
    "CKP_DIR = 'checkpoints'\n",
    "init_epoch = 1\n",
    "\n",
    "ckp = tf.train.latest_checkpoint(CKP_DIR)\n",
    "if ckp:\n",
    "    model.load_weights(ckp)\n",
    "    init_epoch = int(ckp.split('_')[-1]) + 1\n",
    "    print(f'Resume training from epoch {init_epoch-1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00206240",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(inputs):\n",
    "    with tf.GradientTape() as tape:\n",
    "        model(inputs)\n",
    "        c_loss, s_loss = model.losses\n",
    "        loss = c_loss + s_loss\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    c_loss_metric(c_loss)\n",
    "    s_loss_metric(s_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea14dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be2bd327",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(dataset, init_epoch):\n",
    "    for epoch in range(init_epoch, EPOCHS+1):\n",
    "        print(f'Epoch {epoch:>2}/{EPOCHS}')\n",
    "        for step, inputs in enumerate(ds.take(STEPS_PER_EPOCH)):\n",
    "            train_step(inputs)\n",
    "            print(f'{step+1:>5}/{STEPS_PER_EPOCH} - loss: {c_loss_metric.result()+s_loss_metric.result():,.2f} - content loss: {c_loss_metric.result():,.2f} - style loss: {s_loss_metric.result():,.2f}', end='\\r') \n",
    "\n",
    "        print()\n",
    "        model.save_weights(os.path.join(CKP_DIR, f'ckpt_{epoch}'))\n",
    "        c_loss_metric.reset_states()\n",
    "        s_loss_metric.reset_states()\n",
    "\n",
    "        output, c_enc_c, normalized_c, out_enc_c = model((c_batch, s_batch))\n",
    "        plot_outputs((s_batch, c_batch, output, c_enc_c, normalized_c, out_enc_c), \n",
    "                     ('Style', 'Content', 'Trans', 'Content Enc', 'Normalized', 'Trans Enc'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf2a34e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train(ds, init_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a0422c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Data\n",
    "CKP_DIR = 'checkpoints/ckpt_20'\n",
    "\n",
    "model = ArbitraryStyleTransferNet(img_shape=IMG_SHAPE)\n",
    "model.load_weights(CKP_DIR)\n",
    "\n",
    "ds = build_dataset()\n",
    "\n",
    "for idx, (c_batch, s_batch) in enumerate(ds):\n",
    "    if idx > 1:\n",
    "        break\n",
    "    output, c_enc_c, normalized_c, out_enc_c = model((c_batch, s_batch))\n",
    "    print('Recovered loss:', tf.reduce_sum(model.losses).numpy())\n",
    "\n",
    "    plot_outputs((s_batch, c_batch, output), ('Style', 'Content', 'Trans'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d7a57ad",
   "metadata": {},
   "source": [
    "Due to ipynb file size is too big and cannot be uploaded to eeclass, I've decided to delete the output for the train and test data here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676fe5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_example(path, init_shape=(IMG_SHAPE[0], IMG_SHAPE[1])):\n",
    "    image = tf.io.read_file(path)\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = tf.image.resize(image, init_shape)\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    \n",
    "    # Convert image from RGB to BGR, then zero-center each color channel with\n",
    "    # respect to the ImageNet dataset, without scaling.\n",
    "    image = image[..., ::-1] # RGB to BGR\n",
    "    image -= (103.939, 116.779, 123.68) # BGR means\n",
    "    return image\n",
    "\n",
    "def nthu_example(num_gpus=1):\n",
    "    c_paths = ['./dataset/content_nthu.jpg']\n",
    "    \n",
    "    s_paths = []\n",
    "    for s_dir in STYLE_DIRS:\n",
    "        s_paths += Path(s_dir).glob('*.jpg')\n",
    "    s_paths = [str(path.resolve()) for path in s_paths]\n",
    "    print(f'Building dataset from {len(c_paths):,} content images and {len(s_paths):,} style images... ', end='')\n",
    "    \n",
    "    AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "    \n",
    "    c_ds = tf.data.Dataset.from_tensor_slices(c_paths)\n",
    "    c_ds = c_ds.map(preprocess_example, num_parallel_calls=AUTOTUNE)\n",
    "    c_ds = c_ds.repeat()\n",
    "    c_ds = c_ds.shuffle(buffer_size=SHUFFLE_BUFFER)\n",
    "    \n",
    "    s_ds = tf.data.Dataset.from_tensor_slices(s_paths)\n",
    "    s_ds = s_ds.map(preprocess_image, num_parallel_calls=AUTOTUNE)\n",
    "    s_ds = s_ds.repeat()\n",
    "    s_ds = s_ds.shuffle(buffer_size=SHUFFLE_BUFFER)\n",
    "    \n",
    "    ds = tf.data.Dataset.zip((c_ds, s_ds))\n",
    "    ds = ds.batch(BATCH_SIZE * num_gpus)\n",
    "    ds = ds.prefetch(buffer_size=AUTOTUNE)\n",
    "    \n",
    "    print('done')\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c359416",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = nthu_example()\n",
    "outputs = []\n",
    "\n",
    "for idx, (c_batch, s_batch) in enumerate(ds):\n",
    "    if idx > 4:\n",
    "        break\n",
    "    output, c_enc_c, normalized_c, out_enc_c = model((c_batch, s_batch))\n",
    "    outputs.append(output)\n",
    "    \n",
    "plot_outputs((outputs))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
